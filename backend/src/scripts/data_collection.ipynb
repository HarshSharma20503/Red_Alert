{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-t9AAjEWWgF",
        "outputId": "eaaec8a5-72e4-4d9a-df8a-4de203954a25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.2.2)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.13.4)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=48a545e9a0602fdbb0c63ecfc39330dfa4fd856500f708c9ba50dc20c82c9f56\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3340 sha256=c0fca757c0237162046562c4f5a452a277af34aec1cd99d3779bafefeb0ad535\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398382 sha256=7c8e584d86b7b5160e20e8649d0a23e3abbbf576a31384413055655f18b3ba64\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=8799d087710d8ca3bfe1e3fdc002f2083e14cc4dcab1cc22490302b47ba5781f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.0.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install newspaper3k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h95y77OpP1Pr",
        "outputId": "c4c7da4a-d763-4f31-c3f2-47098e55e06f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from newspaper import Article\n",
        "import lxml.html.clean\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import yfinance as yf\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwoP-WvNRdMl"
      },
      "outputs": [],
      "source": [
        "def get_company_symbols():\n",
        "  \"\"\"\n",
        "  Returns list of company Symbols form S&P Index\n",
        "  \"\"\"\n",
        "  wikipedia_data = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
        "  snp_company_list = wikipedia_data[0]\n",
        "  snp_changes = wikipedia_data[1]\n",
        "  snp_ticker_symbols = snp_company_list.Symbol.tolist()\n",
        "  return snp_ticker_symbols\n",
        "\n",
        "\n",
        "def get_company_names():\n",
        "  \"\"\"\n",
        "  Returns list of company names form S&P Index\n",
        "  \"\"\"\n",
        "  wikipedia_data = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
        "  snp_company_list = wikipedia_data[0]\n",
        "  snp_changes = wikipedia_data[1]\n",
        "  snp_ticker_symbols = snp_company_list.Symbol.tolist()\n",
        "  return snp_company_list['Security'].tolist()\n",
        "\n",
        "\n",
        "\n",
        "def get_article_links(company, size=10):\n",
        "  \"\"\"\n",
        "  company-> symbol\n",
        "  size-> Number of links needed\n",
        "\n",
        "  return:\n",
        "  list of links for articles\n",
        "  \"\"\"\n",
        "\n",
        "  url = f\"https://api.queryly.com/cnbc/json.aspx?queryly_key=31a35d40a9a64ab3&query={company}&endindex=40&batchsize={size*3}&callback=&showfaceted=false&timezoneoffset=-330&facetedfields=formats&facetedkey=formats%7C&facetedvalue=!Press%20Release%7C&additionalindexes=4cd6f71fbf22424d,937d600b0d0d4e23,3bfbe40caee7443e,626fdfcd96444f28\"\n",
        "\n",
        "  response = requests.get(url)\n",
        "  links = []\n",
        "  # print(company)\n",
        "  try:\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        for i in data['results']:\n",
        "          if i['cn:type']!='cnbcvideo':\n",
        "            links.append(i['url'])\n",
        "\n",
        "    else:\n",
        "        print(\"Error:\", response.status_code)\n",
        "  except Exception as e:\n",
        "    print(company,':',e)\n",
        "\n",
        "  return links[:size]\n",
        "\n",
        "\n",
        "  #####\n",
        "\n",
        "print(get_article_links('AES Corporation'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_article(url):\n",
        "  \"\"\"\n",
        "  input-> URL of aritcle\n",
        "\n",
        "  Return-> [article.title, article.text, article.publish_date]\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    article = Article(url, language=\"en\")\n",
        "    article.download()\n",
        "    article.parse()\n",
        "    article.nlp()\n",
        "\n",
        "    return [article.title, article.text, article.publish_date]\n",
        "\n",
        "  except Exception as e:\n",
        "    print(url,':',e)\n",
        "    return []\n",
        "\n",
        "\n",
        "\n",
        "#1\n",
        "def get_links_DataFrame():\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        DataFrame['links','symbol','company']\n",
        "    \"\"\"\n",
        "    names = get_company_names()\n",
        "    symbols = get_company_symbols()\n",
        "\n",
        "    final_DataFrame = pd.DataFrame(columns=['links', 'symbol', 'company'])\n",
        "\n",
        "    pbar = tqdm(total=len(names), desc=\"Processing Companies\", unit=\"company\")\n",
        "\n",
        "    for i in range(len(names)):\n",
        "        links = get_article_links(names[i])\n",
        "        new_df = pd.DataFrame(links, columns=['links'])\n",
        "        new_df['symbol'] = symbols[i]\n",
        "        new_df['company'] = names[i]\n",
        "        final_DataFrame = pd.concat([final_DataFrame, new_df], axis=0)\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    return final_DataFrame\n",
        "\n",
        "\n",
        "\n",
        "#2\n",
        "def get_article_data(df):\n",
        "    \"\"\"\n",
        "    df['links'] -> links for all the articles\n",
        "\n",
        "    returns:\n",
        "    returns df with article data[title,txt,publishdate]\n",
        "    \"\"\"\n",
        "    data = []\n",
        "\n",
        "    pbar = tqdm(total=len(df), desc=\"Fetching Article Data\", unit=\"article\")\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        data.append(get_article(df.iloc[i, 0]))\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    new_df = pd.DataFrame(data, columns=['Title', 'Text', 'Publishdate'])\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df = pd.concat([df, new_df], axis=1)\n",
        "    # df['Title']=new_df['Title']\n",
        "    # df['Text']=new_df['Text']\n",
        "    # df['Publishdate']=new_df['Publishdate']\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def price_change(symbol, date, limit=10):\n",
        "    '''\n",
        "    symbol -> company symbol\n",
        "    limit -> for in active market days\n",
        "\n",
        "    '''\n",
        "\n",
        "    if(limit<=0):\n",
        "      return None\n",
        "\n",
        "    stock_data = yf.download(symbol, start=date, end=(pd.to_datetime(date) + pd.Timedelta(days=1)).strftime('%Y-%m-%d'))\n",
        "\n",
        "    if not stock_data.empty:\n",
        "      return stock_data['Close'].iloc[0] - stock_data['Open'].iloc[0]\n",
        "    else:\n",
        "      next_day = (pd.to_datetime(date) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "      return price_change(symbol, next_day, limit -1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def add_stock_change_column(df):\n",
        "    \"\"\"\n",
        "    Calculate stock changes and add a 'Change' column to the DataFrame.\n",
        "\n",
        "    Args:\n",
        "    df (DataFrame): DataFrame containing 'symbol' and 'publishdate' columns.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: DataFrame with an additional 'Change' column.\n",
        "    \"\"\"\n",
        "    changes = []\n",
        "\n",
        "    pbar = tqdm(total=len(df), desc=\"Calculating Changes\", unit=\"article\")\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        change = price_change(df['symbol'][i], df['Publishdate'][i])\n",
        "        changes.append(change)\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    df['Change'] = changes\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def get_articles_data(urls, df,symbol):\n",
        "#     \"\"\"\n",
        "#     Inputs:\n",
        "#       Urls -> list of links for the articles\n",
        "#       df -> df to which the extracted data is to be concartenated\n",
        "#       symbol-> stock symbol of the company\n",
        "#     \"\"\"\n",
        "#     data = []\n",
        "#     for url in urls:\n",
        "#         article_data = get_article(url)\n",
        "#         if article_data:\n",
        "#             data.append(article_data)\n",
        "\n",
        "#     new_df = pd.DataFrame(data, columns=['Title', 'Text', 'PublishDate','symbol'])\n",
        "#     new_df['symbol'] = symbol\n",
        "#     df = pd.concat([df, new_df], axis=1)\n",
        "#     return df\n",
        "\n",
        "\n",
        "def Main():\n",
        "  df = get_links_DataFrame()\n",
        "  df.to_csv('/content/drive/MyDrive/stock_data_links.csv', index=False)\n",
        "  df = get_article_data(df)\n",
        "  df.to_csv('/content/drive/MyDrive/stock_data_Articles.csv', index=False)\n",
        "  df = add_stock_change_column(df)\n",
        "  df.to_csv('/content/drive/MyDrive/stock_data_Complete.csv', index=False)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkVms-Y-Tnwe"
      },
      "outputs": [],
      "source": [
        "companies_names = get_company_names()\n",
        "\n",
        "companies = get_company_symbols()\n",
        "\n",
        "df = Main()\n",
        "\n",
        "df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZSw0XsQ4R_3"
      },
      "outputs": [],
      "source": [
        "# prompt: export the df whihc is created above to the drive/mydrive  index off\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/stock_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rMCxPzOzDen"
      },
      "outputs": [],
      "source": [
        "# # prompt: export data as json file {sy: companies[i],cmp: companies_names[i]}...\n",
        "\n",
        "# import json\n",
        "\n",
        "# data = []\n",
        "# for i in range(len(companies)):\n",
        "#     data.append(companies_names[i])\n",
        "\n",
        "# with open('companies.json', 'w') as outfile:\n",
        "#     json.dump(data, outfile)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}